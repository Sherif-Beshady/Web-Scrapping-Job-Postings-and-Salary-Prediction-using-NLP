{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***Project 4: Web Scraping Job Postings***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 1 - *Web Scrapping Indeed*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import bs4\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status Code:  200\n"
     ]
    }
   ],
   "source": [
    "# you will need the requests library in order to fully utilize bs4\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "# target web page\n",
    "url = \"https://au.indeed.com/jobs?q=data+scientist&l=Australia&radius=100\"\n",
    "# establishing the connection to the webpage\n",
    "response = requests.get(url)\n",
    "\n",
    "# You can use status codes to understand how the target server responds to your request.\n",
    "#Ex. 200 = OK, 400 = Bad Request, 403 = Forbidden, 404 = Not Found\n",
    "print('Status Code: ',response.status_code)\n",
    "\n",
    "# Pull HTML string out of requests and convert to a python string\n",
    "soup = BeautifulSoup(response.text,\"html.parser\")\n",
    "\n",
    "# print(\"\\nFirst part of HTML document fetched as string:\\n\")\n",
    "# print(soup.prettify())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting *Job Title*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_job_title_from_result(soup): \n",
    "        jobs = []\n",
    "        for div in soup.find_all(name='div', attrs={'class':'title'}):\n",
    "            for a in div.find_all(name='a', attrs={'data-tn-element':'jobTitle'}):\n",
    "                jobs.append(a['title'])\n",
    "        return(jobs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting *Company Name*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_company_from_result(soup): \n",
    "    companies = []\n",
    "    \n",
    "    for div in soup.find_all(name='div', attrs={'class':'row'}):\n",
    "        company = div.find_all(name='span', attrs={'class':'company'})\n",
    "        if len(company) > 0:\n",
    "            for b in company:\n",
    "                companies.append(b.text.strip())\n",
    "        else:\n",
    "            sec_try = div.find_all(name='span', attrs={'class':'result-link-source'})\n",
    "            for span in sec_try:\n",
    "                companies.append(span.text.strip())\n",
    "    \n",
    "    return(companies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting *Location*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract_location(soup)\n",
    "def extract_location(soup):\n",
    "    location=[]\n",
    "\n",
    "    for div in soup.find_all('div', attrs={'class':'sjcl'}):\n",
    "        div_one=div.find('div',attrs={'class':'location'})\n",
    "        if div_one==None:\n",
    "            div_two=div.find('span',attrs={'class':'location'})\n",
    "            location.append(div_two.text.strip())\n",
    "        else:\n",
    "            location.append(div_one.text.strip()) \n",
    "            \n",
    "    return(location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_location=extract_location(soup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary Link Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_link_from_result(soup): \n",
    "    links = []\n",
    "    summary=[]\n",
    "    for div in soup.find_all('div', attrs={'class':'title'}):\n",
    "        div_one=div.find(\"a\", {\"class\":\"jobtitle turnstileLink \"})['href']\n",
    "        if div_one==None:\n",
    "            links.append('Nothing_found')\n",
    "        else:\n",
    "            link='https://au.indeed.com/'+div_one\n",
    "            response = requests.get(link)\n",
    "            summary_soup = BeautifulSoup(response.text,\"html.parser\")\n",
    "            summary_soup.find_all('div', attrs={'class':'jobsearch-ViewJobLayout'})\n",
    "            for div in summary_soup.find_all('div', attrs={'id':'jobDescriptionText'}):\n",
    "                summary.append(div.text.strip())    \n",
    "            links.append(link.strip())\n",
    "    df_links=pd.DataFrame(links)\n",
    "    df_summary=pd.DataFrame(summary)\n",
    "    df=pd.concat([df_links,df_summary], axis=1)\n",
    "    df.columns=['Links','Summary']\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 953 ms, sys: 41.1 ms, total: 994 ms\n",
      "Wall time: 5.41 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Links</th>\n",
       "      <th>Summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://au.indeed.com//pagead/clk?mo=r&amp;ad=-6NY...</td>\n",
       "      <td>Flexible work arrangements to meet your needs\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://au.indeed.com//pagead/clk?mo=r&amp;ad=-6NY...</td>\n",
       "      <td>First a bit about ANZ\\n\\nAt ANZ, everything we...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://au.indeed.com//pagead/clk?mo=r&amp;ad=-6NY...</td>\n",
       "      <td>The Team\\n\\nThe Customer Service Operations Au...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://au.indeed.com//pagead/clk?mo=r&amp;ad=-6NY...</td>\n",
       "      <td>An Sydney based company is currently looking f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://au.indeed.com//rc/clk?jk=f3ce2c76078a9...</td>\n",
       "      <td>Job Description\\nData Assimilation Scientist\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>https://au.indeed.com//rc/clk?jk=f3d88d86aa03a...</td>\n",
       "      <td>We are a diverse and highly regarded collectiv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>https://au.indeed.com//rc/clk?jk=95d3c3ee99b0f...</td>\n",
       "      <td>:\\n\\nAs the Customer Data Scientist, you will ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>https://au.indeed.com//rc/clk?jk=324e727f49bb6...</td>\n",
       "      <td>The Opportunity\\nImmerse Yourself in an Inclus...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>https://au.indeed.com//rc/clk?jk=903d5a34c9071...</td>\n",
       "      <td>Deliver components of the design, implementati...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>https://au.indeed.com//rc/clk?jk=03e20b28ddf9f...</td>\n",
       "      <td>Be part of an iconic Australian business\\nGrea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>https://au.indeed.com//company/Private-Family/...</td>\n",
       "      <td>Boutique investment firm is seeking a graduate...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>https://au.indeed.com//rc/clk?jk=e889e5ad2a761...</td>\n",
       "      <td>About us\\nSeer is developing technology to rev...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>https://au.indeed.com//rc/clk?jk=7f8bcd114a737...</td>\n",
       "      <td>First a bit about ANZ\\n\\nAt ANZ, everything we...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>https://au.indeed.com//rc/clk?jk=9627034d8cd27...</td>\n",
       "      <td>Digital Data Scientist World-class learning an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>https://au.indeed.com//pagead/clk?mo=r&amp;ad=-6NY...</td>\n",
       "      <td>Are you curious about how digital is changing ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>https://au.indeed.com//pagead/clk?mo=r&amp;ad=-6NY...</td>\n",
       "      <td>Data Commercialisation is a high-growth busine...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Links  \\\n",
       "0   https://au.indeed.com//pagead/clk?mo=r&ad=-6NY...   \n",
       "1   https://au.indeed.com//pagead/clk?mo=r&ad=-6NY...   \n",
       "2   https://au.indeed.com//pagead/clk?mo=r&ad=-6NY...   \n",
       "3   https://au.indeed.com//pagead/clk?mo=r&ad=-6NY...   \n",
       "4   https://au.indeed.com//rc/clk?jk=f3ce2c76078a9...   \n",
       "5   https://au.indeed.com//rc/clk?jk=f3d88d86aa03a...   \n",
       "6   https://au.indeed.com//rc/clk?jk=95d3c3ee99b0f...   \n",
       "7   https://au.indeed.com//rc/clk?jk=324e727f49bb6...   \n",
       "8   https://au.indeed.com//rc/clk?jk=903d5a34c9071...   \n",
       "9   https://au.indeed.com//rc/clk?jk=03e20b28ddf9f...   \n",
       "10  https://au.indeed.com//company/Private-Family/...   \n",
       "11  https://au.indeed.com//rc/clk?jk=e889e5ad2a761...   \n",
       "12  https://au.indeed.com//rc/clk?jk=7f8bcd114a737...   \n",
       "13  https://au.indeed.com//rc/clk?jk=9627034d8cd27...   \n",
       "14  https://au.indeed.com//pagead/clk?mo=r&ad=-6NY...   \n",
       "15  https://au.indeed.com//pagead/clk?mo=r&ad=-6NY...   \n",
       "\n",
       "                                              Summary  \n",
       "0   Flexible work arrangements to meet your needs\\...  \n",
       "1   First a bit about ANZ\\n\\nAt ANZ, everything we...  \n",
       "2   The Team\\n\\nThe Customer Service Operations Au...  \n",
       "3   An Sydney based company is currently looking f...  \n",
       "4   Job Description\\nData Assimilation Scientist\\n...  \n",
       "5   We are a diverse and highly regarded collectiv...  \n",
       "6   :\\n\\nAs the Customer Data Scientist, you will ...  \n",
       "7   The Opportunity\\nImmerse Yourself in an Inclus...  \n",
       "8   Deliver components of the design, implementati...  \n",
       "9   Be part of an iconic Australian business\\nGrea...  \n",
       "10  Boutique investment firm is seeking a graduate...  \n",
       "11  About us\\nSeer is developing technology to rev...  \n",
       "12  First a bit about ANZ\\n\\nAt ANZ, everything we...  \n",
       "13  Digital Data Scientist World-class learning an...  \n",
       "14  Are you curious about how digital is changing ...  \n",
       "15  Data Commercialisation is a high-growth busine...  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "extract_link_from_result(soup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting *Salaries*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_salary_from_result(soup): \n",
    "    salaries = []\n",
    "    for div in soup.find_all('div', attrs={'class':'title'}):\n",
    "        div_one=div.find('a', attrs={'class':'salary no-wrap'})\n",
    "        if div_one==None:\n",
    "            salaries.append('Nothing_found')\n",
    "        else:\n",
    "            salaries.append(div_one.text.strip())\n",
    "\n",
    "    return salaries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Nothing_found',\n",
       " 'Nothing_found',\n",
       " 'Nothing_found',\n",
       " '$110,000 - $150,000 a year',\n",
       " 'Nothing_found',\n",
       " 'Nothing_found',\n",
       " 'Nothing_found',\n",
       " 'Nothing_found',\n",
       " '$98,000 - $106,000 a year',\n",
       " 'Nothing_found',\n",
       " 'Nothing_found',\n",
       " 'Nothing_found',\n",
       " '$84,019 - $95,329 a year',\n",
       " 'Nothing_found',\n",
       " 'Nothing_found',\n",
       " 'Nothing_found',\n",
       " 'Nothing_found',\n",
       " '$800 - $900 a day']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_salary_from_result(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_summary_from_result(soup): \n",
    "    summaries = []\n",
    "    all_links = soup.find_all(\"a\")\n",
    "    for link in all_links:\n",
    "        print(link.get(\"href\"))\n",
    "        summaries.append(span.text.strip())\n",
    "    return summaries\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### URL construction using different key words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['job_title', 'company_name', 'location','summary'  ,'salary']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['https://au.indeed.com/jobs?q=data+scientist&l=Australia&start=0', 'https://au.indeed.com/jobs?q=intelligence+analyst&l=Australia&start=0', 'https://au.indeed.com/jobs?q=quantitative+analyst&l=Australia&start=0', 'https://au.indeed.com/jobs?q=data+engineer&l=Australia&start=0', 'https://au.indeed.com/jobs?q=business+intelligence&l=Australia&start=0', 'https://au.indeed.com/jobs?q=analytics&l=Australia&start=0']\n",
      "['625', '543', '287', '2882', '1740', '5155']\n"
     ]
    }
   ],
   "source": [
    "urls=[]\n",
    "job_titles=['data+scientist', 'intelligence+analyst','quantitative+analyst','data+engineer','business+intelligence','analytics']\n",
    "clean_search_results=[]\n",
    "for title in job_titles:\n",
    "    \n",
    "    url ='https://au.indeed.com/jobs?q='+title+'&l=Australia'+'&start=0'\n",
    "    response = requests.get(url)\n",
    "    if response.status_code==200:\n",
    "        urls.append(url)\n",
    "        soup = BeautifulSoup(response.text,\"html.parser\")\n",
    "        search_results=soup.find('div', attrs={'id': 'searchCount'}).text.strip()\n",
    "        search_results=search_results.split()\n",
    "        clean_search_results.append(str(search_results[3]))\n",
    "        \n",
    "clean_search_results=[clean_search_results[i].replace(',','') for i in range(0,len(clean_search_results),1)]\n",
    "print(urls)\n",
    "print (clean_search_results)\n",
    "b=enumerate(urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<enumerate object at 0x11a3bf048>\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://au.indeed.com/jobs?q=data+scientist&l=Australia&start=0\n",
      "625\n",
      "https://au.indeed.com/jobs?q=intelligence+analyst&l=Australia&start=0\n",
      "543\n",
      "https://au.indeed.com/jobs?q=quantitative+analyst&l=Australia&start=0\n",
      "287\n",
      "https://au.indeed.com/jobs?q=data+engineer&l=Australia&start=0\n",
      "2882\n",
      "https://au.indeed.com/jobs?q=business+intelligence&l=Australia&start=0\n",
      "1740\n",
      "https://au.indeed.com/jobs?q=analytics&l=Australia&start=0\n",
      "5155\n"
     ]
    }
   ],
   "source": [
    "def urls_list():\n",
    "    urls=[]\n",
    "    clean_search_results=[]\n",
    "    job_titles=['data+scientist', 'intelligence+analyst','quantitative+analyst','data+engineer','business+intelligence','analytics',\n",
    "                'performance+analyst','statistics']\n",
    "    columns = ['job_title', 'company_name', 'location','summary'  ,'salary']\n",
    "    \n",
    "    for title in job_titles:\n",
    "        url ='https://au.indeed.com/jobs?q='+title+'&l=Australia'+'&start='\n",
    "        #search for different Url combinations\n",
    "        response = requests.get(url)\n",
    "        if response.status_code==200:\n",
    "            #make sure the url work \n",
    "            urls.append(url)\n",
    "            soup = BeautifulSoup(response.text,\"html.parser\")\n",
    "            search_results=soup.find('div', attrs={'id': 'searchCount'}).text.strip()\n",
    "            search_results=search_results.split()\n",
    "            clean_search_results.append(str(search_results[3]))\n",
    "            \n",
    "    clean_search_results=[clean_search_results[i].replace(',','') for i in range(0,len(clean_search_results),1)]\n",
    "    \n",
    "    return clean_search_results,urls\n",
    "\n",
    "for i in range(0,len(urls)):\n",
    "    job_title = urls[i]\n",
    "    search_items=clean_search_results[i]\n",
    "    print(job_title)\n",
    "    print(search_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.setrecursionlimit(1500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Scrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    376\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Python 2.7, use buffering of HTTP responses\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 377\u001b[0;31m                 \u001b[0mhttplib_response\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetresponse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuffering\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    378\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Python 3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: getresponse() got an unexpected keyword argument 'buffering'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/requests/api.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(url, params, **kwargs)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetdefault\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'allow_redirects'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'get'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/requests/api.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;31m# cases, and look like a memory leak in others.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0msessions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/requests/sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    531\u001b[0m         }\n\u001b[1;32m    532\u001b[0m         \u001b[0msend_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 533\u001b[0;31m         \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0msend_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    534\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    535\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/requests/sessions.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    644\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    645\u001b[0m         \u001b[0;31m# Send the request\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 646\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madapter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    647\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    648\u001b[0m         \u001b[0;31m# Total elapsed time of the request (approximately)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/requests/adapters.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    447\u001b[0m                     \u001b[0mdecode_content\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m                     \u001b[0mretries\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_retries\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 449\u001b[0;31m                     \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    450\u001b[0m                 )\n\u001b[1;32m    451\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    598\u001b[0m                                                   \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout_obj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    599\u001b[0m                                                   \u001b[0mbody\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 600\u001b[0;31m                                                   chunked=chunked)\n\u001b[0m\u001b[1;32m    601\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    602\u001b[0m             \u001b[0;31m# If we're going to release the connection in ``finally:``, then\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    378\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Python 3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    379\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 380\u001b[0;31m                     \u001b[0mhttplib_response\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetresponse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    381\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    382\u001b[0m                     \u001b[0;31m# Remove the TypeError from the exception chain in Python 3;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/http/client.py\u001b[0m in \u001b[0;36mgetresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1319\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1321\u001b[0;31m                 \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbegin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1322\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mConnectionError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/http/client.py\u001b[0m in \u001b[0;36mbegin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    294\u001b[0m         \u001b[0;31m# read until we get a non-100 response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 296\u001b[0;31m             \u001b[0mversion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreason\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    297\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mstatus\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mCONTINUE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/http/client.py\u001b[0m in \u001b[0;36m_read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    255\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_read_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 257\u001b[0;31m         \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_MAXLINE\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"iso-8859-1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    258\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0m_MAXLINE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mLineTooLong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"status line\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    587\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 589\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    590\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    591\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/urllib3/contrib/pyopenssl.py\u001b[0m in \u001b[0;36mrecv_into\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    292\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 294\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    295\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mOpenSSL\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSSL\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSysCallError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msuppress_ragged_eofs\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Unexpected EOF'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/OpenSSL/SSL.py\u001b[0m in \u001b[0;36mrecv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1811\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_lib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSSL_peek\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ssl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnbytes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1812\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1813\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_lib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSSL_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ssl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnbytes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1814\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_raise_ssl_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ssl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1815\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "def urls_list(jobs):\n",
    "    urls=[]\n",
    "    clean_search_results=[]\n",
    "    job_titles=['data+scientist', 'intelligence+analyst','quantitative+analyst','data+engineer','business+intelligence+analyst',\n",
    "                'performance+analyst']\n",
    "    columns = ['job_title', 'company_name', 'location','summary'  ,'salary']\n",
    "    \n",
    "    for title in job_titles:\n",
    "        \n",
    "        #search for different Url combinations\n",
    "        url ='https://au.indeed.com/jobs?q='+title+'&l=Australia'+'&start='\n",
    "        response = requests.get(url)\n",
    "        if response.status_code==200:\n",
    "            #make sure the url work \n",
    "            urls.append(url)\n",
    "            soup = BeautifulSoup(response.text,\"html.parser\")\n",
    "            search_results=soup.find('div', attrs={'id': 'searchCount'}).text.strip()\n",
    "            search_results=search_results.split()\n",
    "            clean_search_results.append(str(search_results[3]))\n",
    "            \n",
    "    clean_search_results=[clean_search_results[i].replace(',','') for i in range(0,len(clean_search_results),1)]\n",
    "    \n",
    "    return clean_search_results,urls\n",
    "#----------------------------------------------------------------------------------------------------------------------------------------#\n",
    "\n",
    "for i in range(0,len(urls)):\n",
    "    job_title = urls[i]\n",
    "    search_items=clean_search_results[i]\n",
    "\n",
    "    #Start scrappring the  working url\n",
    "    db=[]\n",
    "\n",
    "    summary_soup=[]\n",
    "    for page in np.arange(0, int(search_items), 10):\n",
    "        page = requests.get(job_title+str(page))\n",
    "\n",
    "        time.sleep(2)  #ensuring at least 1 second between page grabs        \n",
    "        soup = BeautifulSoup(page.text,\"html.parser\")\n",
    "        summary_soup.append(soup)\n",
    "        \n",
    "        for div in soup.find_all(name='div', attrs={'class':'row'}):\n",
    "            \n",
    "            #creating an empty list to hold the data for each posting\n",
    "            job_post = [] \n",
    "            \n",
    "            #grabbing job title\n",
    "            for a in div.find_all(name='a', attrs={'data-tn-element':'jobTitle'}):\n",
    "                job_post.append(a['title'])\n",
    "\n",
    "            #grabbing company name\n",
    "            company = div.find_all(name='span', attrs={'class':'company'})\n",
    "            if len(company) > 0:\n",
    "                for b in company:\n",
    "                     job_post.append(b.text.strip())\n",
    "            else:\n",
    "                sec_try = div.find_all(name='span', attrs={'class':'result-link-source'})\n",
    "                for span in sec_try:\n",
    "                    job_post.append(span.text.strip())\n",
    "\n",
    "            #grabbing location name\n",
    "            div_one=div.find('div',attrs={'class':'location'})\n",
    "            if div_one==None:\n",
    "                div_two=div.find('span',attrs={'class':'location'})\n",
    "                job_post.append(div_two.text.strip())\n",
    "            else:\n",
    "                job_post.append(div_one.text.strip()) \n",
    "                \n",
    "            #appending our main list with the results           \n",
    "        db.append(job_post)\n",
    "    \n",
    "    #converting results to DF\n",
    "    df_data=pd.DataFrame(db)\n",
    "\n",
    "#----------------------------------------------------------------------------------------------------------------------------------------#\n",
    "   \n",
    " #saving DF as a local csv file\n",
    "\n",
    "df_data.to_csv(\"/Users/sherf/Desktop/GA Units/GA_P4/Indeed_Scrapper_data.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6920, 5)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Part 1 Comments:*\n",
    "\n",
    "---\n",
    "In this section I managed to scrap Indeed website extracting 6920 jobs and saved these jobs to a CSV file to be used in further processing.\n",
    "The scrapping code took 45 minutes to run. I didn't scrap complete job description only the job summary and will try to make the best use of that."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
