{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### P4_Section 2 - *Processing Scrapped Data*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=RuntimeWarning)\n",
    "from sklearn import metrics\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from textblob import TextBlob, Word\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/sherf\n"
     ]
    }
   ],
   "source": [
    "cd /Users/sherf/ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 2 - Part 1 - Data Preprocessing\n",
    "\n",
    "- In this section we load the scrapped data and perform simple EDA as the following:\n",
    "\n",
    "1- Add column names\n",
    "\n",
    "2- Sort raws descending based on *comapny name*\n",
    "\n",
    "3- Drop duplicate values\n",
    "\n",
    "4- In *Salary* column we replace all the nan with nothing found\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Index</th>\n",
       "      <th>job_title</th>\n",
       "      <th>salary</th>\n",
       "      <th>company_name</th>\n",
       "      <th>location</th>\n",
       "      <th>summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4515</th>\n",
       "      <td>4515</td>\n",
       "      <td>LEGEND SEM | PPC Specialist</td>\n",
       "      <td>$30 - $60 an hour</td>\n",
       "      <td>wildminds agency</td>\n",
       "      <td>Windsor VIC</td>\n",
       "      <td>*Like to push the boundaries with digital mark...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3289</th>\n",
       "      <td>3289</td>\n",
       "      <td>ACCOUNT MANAGERS</td>\n",
       "      <td>NaN</td>\n",
       "      <td>uberbrand</td>\n",
       "      <td>Sydney NSW</td>\n",
       "      <td>About this role You often read about roles bei...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1422</th>\n",
       "      <td>1422</td>\n",
       "      <td>Business Analyst   Power BI</td>\n",
       "      <td>NaN</td>\n",
       "      <td>u&amp;u Recruitment Partners</td>\n",
       "      <td>Sydney Western Suburbs NSW</td>\n",
       "      <td>Use your business analysis and technical BI sk...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2272</th>\n",
       "      <td>2272</td>\n",
       "      <td>iOS QA Engineer</td>\n",
       "      <td>NaN</td>\n",
       "      <td>real time</td>\n",
       "      <td>Adelaide SA</td>\n",
       "      <td>About the company:Real Time Data (RTD) is a pi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1484</th>\n",
       "      <td>1484</td>\n",
       "      <td>Litigation Support Analyst</td>\n",
       "      <td>NaN</td>\n",
       "      <td>people2people</td>\n",
       "      <td>Sydney NSW</td>\n",
       "      <td>THE FIRM This global firm is a renowned law fi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Index                    job_title             salary  \\\n",
       "4515   4515  LEGEND SEM | PPC Specialist  $30 - $60 an hour   \n",
       "3289   3289             ACCOUNT MANAGERS                NaN   \n",
       "1422   1422  Business Analyst   Power BI                NaN   \n",
       "2272   2272              iOS QA Engineer                NaN   \n",
       "1484   1484   Litigation Support Analyst                NaN   \n",
       "\n",
       "                  company_name                    location  \\\n",
       "4515          wildminds agency                 Windsor VIC   \n",
       "3289                 uberbrand                  Sydney NSW   \n",
       "1422  u&u Recruitment Partners  Sydney Western Suburbs NSW   \n",
       "2272                 real time                 Adelaide SA   \n",
       "1484             people2people                  Sydney NSW   \n",
       "\n",
       "                                                summary  \n",
       "4515  *Like to push the boundaries with digital mark...  \n",
       "3289  About this role You often read about roles bei...  \n",
       "1422  Use your business analysis and technical BI sk...  \n",
       "2272  About the company:Real Time Data (RTD) is a pi...  \n",
       "1484  THE FIRM This global firm is a renowned law fi...  "
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_csv(\"./Downloads/indeed_analyst_k.csv\")\n",
    "columns = ['Index','job_title', 'salary','company_name', 'location','summary']\n",
    "df.columns=columns\n",
    "df.sort_values(['company_name'],axis=0,ascending=False,inplace=True)\n",
    "jobs_list=df.drop_duplicates(subset=['job_title','location'])\n",
    "jobs_list.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sherf/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "jobs_list['salary']=jobs_list['salary'].apply(lambda x:str(x).replace('nan','Nothing found'))\n",
    "jobs_list=jobs_list.reset_index(drop=True)\n",
    "jobs_list.summary.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "# jobs_list['salary'].apply(pd.Series.value_counts).nunique()\n",
    "# jobs_list.sort_values(['salary'],axis=0,ascending=False).nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using string library to remove all unwanted special characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "st=string.punctuation\n",
    "st"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In the section below I have replaced the punctuation on two steps:\n",
    "\n",
    "1- Manually to control the replacement of characters\n",
    "\n",
    "2- Automatic to remove the rest of unwanted characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 525 ms, sys: 21.7 ms, total: 547 ms\n",
      "Wall time: 557 ms\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "columns=jobs_list.columns\n",
    "for i in columns:\n",
    "    jobs_list[i]=jobs_list[i].apply(lambda x:str(x).replace(',',''))\n",
    "    jobs_list[i]=jobs_list[i].apply(lambda x:str(x).replace('$',' '))\n",
    "    jobs_list[i]=jobs_list[i].apply(lambda x:str(x).replace('-',' '))\n",
    "    jobs_list[i]=jobs_list[i].apply(lambda x:str(x).replace('_',' '))\n",
    "    jobs_list[i]=jobs_list[i].apply(lambda x:str(x).replace(';',' '))\n",
    "    jobs_list[i]=jobs_list[i].apply(lambda x:str(x).replace('|',' '))\n",
    "    jobs_list[i]=jobs_list[i].apply(lambda x:str(x).replace('&',' and '))\n",
    "    for j in st:\n",
    "        jobs_list[i]=jobs_list[i].apply(lambda x:str(x).replace(j,''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Like to push the boundaries with digital marketing and SEM campaignswildminds is a creative ideas first agency We’re a bit smarter  bit faster  more cunning than most We work with businesses to get them out of the digital wilderness  with killer ideas and marketing servicesWe’re a small team of great minds who don’t mind life on the quirky side We like to push boundaries  encourage ideas and input  we work as a team but also operate solo to get the job done We’re looking for a epic SEM guru who knows their way around Adwords  InDesign  and all things digital marketingTo be successful in this role you will have demonstrable experience driving exceptional outcomes through Google AdWords  Google Tag Manager GTM  Google Display Network  YouTube and Paid Social You will also be well versed with Google Analytics with the ability to use data driven marketing to make key decisions on clients websites in terms of improving sales and lead generationDo you have what we wantA minimum of 2 3 years experience in managing Paid Media campaignsManaging budgets and bids for multiple platforms including Google AdWords  Bing Ads  Facebook  YouTube  etcBuilding effective FacebookPaid Search campaignsManaging RemarketingDisplay campaigns in Google Display Network and third party platformsConducting  measuring  and implementing the results of AB tests for ad copy and targetingPerforming traffic analysis in Google Analytics related to paid digital traffic and other sourcesmediumsEvaluating budget levels and provide recommendationsMeasuring and optimising campaigns to meet KPIsActing as one of the resident digital marketing expertsProviding guidance  support  and training to junior SEM specialistsPlanning  preparing  and delivering digital marketing reviewsAnalyse clients business goals increase leads  higher search visibility  specific model promotion  etc and translate to a digital marketing proposalStrong strategic  analytical and problem solving skillsThrives under pressureWant bonus pointsYoure Google AdWords certifiedYoure Google Analytics certifiedYou understand current digital trends and consumer needsWhats in it for youYoull be working amongst like minded people in an entrepreneurial environment at our sought after Windsor location With flexible work hours and a possible gym membership on offer   the perks are varied and manyNO RECUITERS   RECUITERS Do not contact us   thanksJob Type ContractSalary  3000 to  6000 hourExperienceGoogle AdWords 1 year RequiredPPC 1 year Preferred'"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jobs_list.summary.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1712"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Extracting jobs without salary\n",
    "salary_list=jobs_list.drop(jobs_list[jobs_list['salary'] !='Nothing found'].index)\n",
    "len(salary_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In the section below I have tackled all the different salary types and converted all to annual salary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "#finding salary catagories\n",
    "hourly_rate=jobs_list[jobs_list['salary'].str.contains(\"hour\")==True]\n",
    "daily_rate=jobs_list[jobs_list['salary'].str.contains(\"day\")==True]\n",
    "weekly_rate=jobs_list[jobs_list['salary'].str.contains(\"week\")==True]\n",
    "yearly_rate=jobs_list[jobs_list['salary'].str.contains(\"year\")==True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def salary_avg(salary_rate):\n",
    "    salary_avg=[]\n",
    "    for i in range(0,(len(salary_rate))):\n",
    "        b=salary_rate['salary'].iloc[i]\n",
    "        a=[int(s) for s in b.split() if s.isdigit()]\n",
    "        avg_salary=np.mean(a)\n",
    "        salary_avg.append(avg_salary)\n",
    "    return salary_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sherf/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "/Users/sherf/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n",
      "/Users/sherf/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  after removing the cwd from sys.path.\n",
      "/Users/sherf/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"\n",
      "/Users/sherf/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  import sys\n",
      "/Users/sherf/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n",
      "/Users/sherf/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    }
   ],
   "source": [
    "hourly_rate['avg_annual_salary']=salary_avg(hourly_rate)\n",
    "hourly_rate['avg_annual_salary']=hourly_rate['avg_annual_salary'].apply(lambda x:np.multiply(x,2080))\n",
    "\n",
    "daily_rate['avg_annual_salary']=salary_avg(daily_rate)\n",
    "daily_rate['avg_annual_salary']=daily_rate['avg_annual_salary'].apply(lambda x:np.multiply(x,260))\n",
    "\n",
    "weekly_rate['avg_annual_salary']=salary_avg(weekly_rate)\n",
    "weekly_rate['avg_annual_salary']=weekly_rate['avg_annual_salary'].apply(lambda x:np.multiply(x,52))\n",
    "\n",
    "yearly_rate['avg_annual_salary']=salary_avg(yearly_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Create a dataframe consist of all jobs with salary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs_with_salary=hourly_rate.append(daily_rate)\n",
    "jobs_with_salary=jobs_with_salary.append(weekly_rate)\n",
    "jobs_with_salary=jobs_with_salary.append(yearly_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min Salary = 17420.0\n",
      "Max Salary = 8987680.0\n"
     ]
    }
   ],
   "source": [
    "print('Min Salary =',jobs_with_salary.avg_annual_salary.min())\n",
    "print('Max Salary =',jobs_with_salary.avg_annual_salary.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 2 - Part 2 - Understanding Factors that impact salary\n",
    "\n",
    "To better understand salary, We divide it into three *BALANCED* classes: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cat 0 - *less Than 85k* \n",
    "\n",
    "Cat 1 - *Between 85k and 120k*\n",
    "\n",
    "Cat 2 - *More than 120k*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sherf/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n",
      "/Users/sherf/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"\n",
      "/Users/sherf/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "Cat_0=jobs_with_salary[(jobs_with_salary.avg_annual_salary.values <=85000)]\n",
    "Cat_0['salary_catagory']=0\n",
    "                       \n",
    "Cat_1=jobs_with_salary[(jobs_with_salary.avg_annual_salary>85000) & (jobs_with_salary.avg_annual_salary<=120000)]\n",
    "Cat_1['salary_catagory']=1\n",
    "                                                                                                      \n",
    "Cat_2=jobs_with_salary[(jobs_with_salary.avg_annual_salary.values >120000)]\n",
    "Cat_2['salary_catagory']=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Index</th>\n",
       "      <th>job_title</th>\n",
       "      <th>salary</th>\n",
       "      <th>company_name</th>\n",
       "      <th>location</th>\n",
       "      <th>summary</th>\n",
       "      <th>avg_annual_salary</th>\n",
       "      <th>salary_catagory</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>662</td>\n",
       "      <td>Clinical Trials Nurse</td>\n",
       "      <td>3158    4434 an hour</td>\n",
       "      <td>Western Sydney Local Health District</td>\n",
       "      <td>Sydney Western Suburbs NSW</td>\n",
       "      <td>Employment Type Temporary Full Time Up to 12 m...</td>\n",
       "      <td>7895680.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>347</th>\n",
       "      <td>1394</td>\n",
       "      <td>Database Business Analyst</td>\n",
       "      <td>70    90 an hour</td>\n",
       "      <td>Talent International</td>\n",
       "      <td>Canberra ACT</td>\n",
       "      <td>Oracle Database  SQL Server 12 month contract ...</td>\n",
       "      <td>166400.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>831</th>\n",
       "      <td>222</td>\n",
       "      <td>Medical Laboratory Scientist</td>\n",
       "      <td>62 an hour</td>\n",
       "      <td>NSW Health Pathology</td>\n",
       "      <td>Sydney NSW</td>\n",
       "      <td>Employment Type Temporary Full Time Position C...</td>\n",
       "      <td>128960.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>949</th>\n",
       "      <td>1556</td>\n",
       "      <td>FP A Manager   Global Retail   6 Month Contract</td>\n",
       "      <td>70    80 an hour</td>\n",
       "      <td>Michael Page</td>\n",
       "      <td>Sydney NSW</td>\n",
       "      <td>6 month contract with a leading cosmetics bran...</td>\n",
       "      <td>156000.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1241</th>\n",
       "      <td>1401</td>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>110    115 an hour</td>\n",
       "      <td>Hudson</td>\n",
       "      <td>Canberra ACT</td>\n",
       "      <td>12 month contract role with possible extension...</td>\n",
       "      <td>234000.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Index                                        job_title  \\\n",
       "109    662                            Clinical Trials Nurse   \n",
       "347   1394                        Database Business Analyst   \n",
       "831    222                     Medical Laboratory Scientist   \n",
       "949   1556  FP A Manager   Global Retail   6 Month Contract   \n",
       "1241  1401                                     Data Analyst   \n",
       "\n",
       "                     salary                          company_name  \\\n",
       "109    3158    4434 an hour  Western Sydney Local Health District   \n",
       "347        70    90 an hour                  Talent International   \n",
       "831              62 an hour                  NSW Health Pathology   \n",
       "949        70    80 an hour                          Michael Page   \n",
       "1241     110    115 an hour                                Hudson   \n",
       "\n",
       "                        location  \\\n",
       "109   Sydney Western Suburbs NSW   \n",
       "347                 Canberra ACT   \n",
       "831                   Sydney NSW   \n",
       "949                   Sydney NSW   \n",
       "1241                Canberra ACT   \n",
       "\n",
       "                                                summary  avg_annual_salary  \\\n",
       "109   Employment Type Temporary Full Time Up to 12 m...          7895680.0   \n",
       "347   Oracle Database  SQL Server 12 month contract ...           166400.0   \n",
       "831   Employment Type Temporary Full Time Position C...           128960.0   \n",
       "949   6 month contract with a leading cosmetics bran...           156000.0   \n",
       "1241  12 month contract role with possible extension...           234000.0   \n",
       "\n",
       "      salary_catagory  \n",
       "109                 2  \n",
       "347                 2  \n",
       "831                 2  \n",
       "949                 2  \n",
       "1241                2  "
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Cat_2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Joining salary classes to form single df\n",
    "jobs_with_salary_cat=Cat_0.append(Cat_1).append(Cat_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    0.348790\n",
       "2    0.336694\n",
       "0    0.314516\n",
       "Name: salary_catagory, dtype: float64"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking to make sure we have a balance in the different classes\n",
    "jobs_with_salary_cat.salary_catagory.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(496, 8)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jobs_with_salary_cat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>job_title</th>\n",
       "      <th>salary</th>\n",
       "      <th>company_name</th>\n",
       "      <th>location</th>\n",
       "      <th>summary</th>\n",
       "      <th>avg_annual_salary</th>\n",
       "      <th>salary_catagory</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Demand Planner</td>\n",
       "      <td>38    40 an hour</td>\n",
       "      <td>people2people</td>\n",
       "      <td>Sydney NSW</td>\n",
       "      <td>The company is a global leader in technology  ...</td>\n",
       "      <td>81120.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Marketing Angel</td>\n",
       "      <td>25 an hour</td>\n",
       "      <td>Impact HR</td>\n",
       "      <td>Northern Beaches NSW</td>\n",
       "      <td>Who is Impact HRPassionate about business’ peo...</td>\n",
       "      <td>52000.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Lodgement Delivery Officers Wanted</td>\n",
       "      <td>30    40 an hour</td>\n",
       "      <td>Hudson</td>\n",
       "      <td>Brisbane QLD</td>\n",
       "      <td>Join a Leading Federal Government Department  ...</td>\n",
       "      <td>72800.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Project Administration</td>\n",
       "      <td>28 an hour</td>\n",
       "      <td>HAYS</td>\n",
       "      <td>Melbourne City Centre VIC</td>\n",
       "      <td>5 6 week temp assignment  travelling to variou...</td>\n",
       "      <td>58240.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Admin Assistant</td>\n",
       "      <td>29 an hour</td>\n",
       "      <td>HAYS</td>\n",
       "      <td>Melbourne City Centre VIC</td>\n",
       "      <td>Temp to perm admin job available in Melbourne ...</td>\n",
       "      <td>60320.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             job_title             salary   company_name  \\\n",
       "0                       Demand Planner   38    40 an hour  people2people   \n",
       "1                      Marketing Angel         25 an hour      Impact HR   \n",
       "2  Lodgement Delivery Officers Wanted    30    40 an hour         Hudson   \n",
       "3               Project Administration         28 an hour           HAYS   \n",
       "4                      Admin Assistant         29 an hour           HAYS   \n",
       "\n",
       "                    location  \\\n",
       "0                 Sydney NSW   \n",
       "1       Northern Beaches NSW   \n",
       "2               Brisbane QLD   \n",
       "3  Melbourne City Centre VIC   \n",
       "4  Melbourne City Centre VIC   \n",
       "\n",
       "                                             summary  avg_annual_salary  \\\n",
       "0  The company is a global leader in technology  ...            81120.0   \n",
       "1  Who is Impact HRPassionate about business’ peo...            52000.0   \n",
       "2  Join a Leading Federal Government Department  ...            72800.0   \n",
       "3  5 6 week temp assignment  travelling to variou...            58240.0   \n",
       "4  Temp to perm admin job available in Melbourne ...            60320.0   \n",
       "\n",
       "   salary_catagory  \n",
       "0                0  \n",
       "1                0  \n",
       "2                0  \n",
       "3                0  \n",
       "4                0  "
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#droppind index column and resetting Index\n",
    "jobs_with_salary_cat.drop(axis=1,columns='Index',inplace=True)\n",
    "jobs_with_salary_cat=jobs_with_salary_cat.reset_index(drop=True)\n",
    "jobs_with_salary_cat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Like to push the boundaries with digital marketing and SEM campaigns wildminds is a creative ideas first agency  We’re a bit smarter  bit faster  more cunning than most  We work with businesses to get them out of the digital wilderness  with killer ideas and marketing services We’re a small team of great minds who don’t mind life on the quirky side  We like to push boundaries  encourage ideas and input  we work as a team but also operate solo to get the job done  We’re looking for a epic SEM guru who knows their way around Adwords  InDesign  and all things digital marketing To be successful in this role you will have demonstrable experience driving exceptional outcomes through Google AdWords  Google Tag Manager GTM  Google Display Network  YouTube and Paid Social  You will also be well versed with Google Analytics with the ability to use data driven marketing to make key decisions on clients  websites in terms of improving sales and lead generation Do you have what we want A minimum of 2 3 years  experience in managing Paid Media campaignsManaging budgets and bids for multiple platforms including Google AdWords  Bing Ads  Facebook  YouTube  etc Building effective Facebook Paid Search campaigns Managing Remarketing Display campaigns in Google Display Network and third party platforms Conducting  measuring  and implementing the results of A B tests for ad copy and targeting Performing traffic analysis in Google Analytics related to paid digital traffic and other sources mediums Evaluating budget levels and provide recommendations Measuring and optimising campaigns to meet KPIs Acting as one of the resident digital marketing experts Providing guidance  support  and training to junior SEM specialists Planning  preparing  and delivering digital marketing reviews Analyse clients business goals  increase leads  higher search visibility  specific model promotion  etc  and translate to a digital marketing proposal Strong strategic  analytical and problem solving skillsThrives under pressureWant bonus points You re Google AdWords certifiedYou re Google Analytics certifiedYou understand current digital trends and consumer needsWhat s in it for you You ll be working amongst like minded people in an entrepreneurial environment at our sought after Windsor location  With flexible work hours and a possible gym membership on offer   the perks are varied and many   NO RECUITERS   RECUITERS Do not contact us   thanksJob Type  ContractSalary   30 00 to  60 00  hourExperience Google AdWords  1 year  Required PPC  1 year  Preferred \n"
     ]
    }
   ],
   "source": [
    "# print the first review\n",
    "print(jobs_with_salary_cat.summary[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Blob & Lemmatiztion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function creates a text blob and lemmatize text\n",
    "\n",
    "def split_into_lemmas(text):\n",
    "    text = str(text).lower()\n",
    "    words = TextBlob(text).words\n",
    "    return [word.lemmatize() for word in words]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 2 - Part 3 - Building a classification Model to classify Salary Class based on Summary text\n",
    "\n",
    "- In this section we will build a classifcation model to classify salary based on job summary\n",
    "\n",
    "- First we will use Naive Bayes Classifier with TfidfVectorizer and CountVectorizer to comapre performance\n",
    "\n",
    "- Second we will feed our data Sequentially to a pipline to try a list of transforms and estimators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 2 - Part 3.1 - classification Models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.37 (+/- 0.04)\n",
      "Accuracy:  0.44953742640874683\n",
      "Accuracy:  0.44\n"
     ]
    }
   ],
   "source": [
    "#Instantiate TfidfVectorizer\n",
    "tf = TfidfVectorizer(analyzer=split_into_lemmas)\n",
    "\n",
    "# Creating the Bag of Words model\n",
    "X = tf.fit_transform(jobs_with_salary_cat.summary).toarray()\n",
    "y = jobs_with_salary_cat.iloc[:,6].values\n",
    "\n",
    "# Splitting the dataset into the Training set and Test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2 )\n",
    "\n",
    "# Fitting Naive Bayes to the Training set\n",
    "classifier = GaussianNB()\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "# Predicting the Test set results\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "#Model Validation\n",
    "scores=cross_val_score(classifier,X,y,cv=5)\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))\n",
    "\n",
    "print('Accuracy: ', metrics.balanced_accuracy_score(y_test, y_pred))\n",
    "print('Accuracy: ', metrics.accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.38 (+/- 0.07)\n",
      "Accuracy:  0.5710041592394534\n",
      "Accuracy:  0.57\n"
     ]
    }
   ],
   "source": [
    "#Instantiate CountVectorizer\n",
    "cv = CountVectorizer(analyzer=split_into_lemmas)\n",
    "\n",
    "# Creating the Bag of Words model\n",
    "X = cv.fit_transform(jobs_with_salary_cat.summary).toarray()\n",
    "y = jobs_with_salary_cat.iloc[:,6].values\n",
    "\n",
    "# Splitting the dataset into the Training set and Test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2 )\n",
    "\n",
    "# Fitting Naive Bayes to the Training set\n",
    "classifier = GaussianNB()\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "# Predicting the Test set results\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "#Model Validation\n",
    "scores=cross_val_score(classifier,X,y,cv=5)\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))\n",
    "\n",
    "print('Accuracy: ', metrics.balanced_accuracy_score(y_test, y_pred))\n",
    "print('Accuracy: ', metrics.accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**It seems that CountVectorizer is giving us better results than TfidfVectorizer**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing model on unseen document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 2, ..., 2, 2, 1])"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_new=cv.transform(jobs_list.summary).toarray()\n",
    "\n",
    "y_pred2 = classifier.predict(X_new)\n",
    "\n",
    "y_pred2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Index</th>\n",
       "      <th>job_title</th>\n",
       "      <th>salary</th>\n",
       "      <th>company_name</th>\n",
       "      <th>location</th>\n",
       "      <th>summary</th>\n",
       "      <th>Model Generalization Results</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4515</td>\n",
       "      <td>LEGEND SEM   PPC Specialist</td>\n",
       "      <td>30    60 an hour</td>\n",
       "      <td>wildminds agency</td>\n",
       "      <td>Windsor VIC</td>\n",
       "      <td>Like to push the boundaries with digital marke...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3289</td>\n",
       "      <td>ACCOUNT MANAGERS</td>\n",
       "      <td>Nothing found</td>\n",
       "      <td>uberbrand</td>\n",
       "      <td>Sydney NSW</td>\n",
       "      <td>About this role You often read about roles bei...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1422</td>\n",
       "      <td>Business Analyst   Power BI</td>\n",
       "      <td>Nothing found</td>\n",
       "      <td>u and u Recruitment Partners</td>\n",
       "      <td>Sydney Western Suburbs NSW</td>\n",
       "      <td>Use your business analysis and technical BI sk...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2272</td>\n",
       "      <td>iOS QA Engineer</td>\n",
       "      <td>Nothing found</td>\n",
       "      <td>real time</td>\n",
       "      <td>Adelaide SA</td>\n",
       "      <td>About the companyReal Time Data RTD is a pione...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1484</td>\n",
       "      <td>Litigation Support Analyst</td>\n",
       "      <td>Nothing found</td>\n",
       "      <td>people2people</td>\n",
       "      <td>Sydney NSW</td>\n",
       "      <td>THE FIRM This global firm is a renowned law fi...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Index                    job_title             salary  \\\n",
       "0  4515  LEGEND SEM   PPC Specialist   30    60 an hour   \n",
       "1  3289             ACCOUNT MANAGERS      Nothing found   \n",
       "2  1422  Business Analyst   Power BI      Nothing found   \n",
       "3  2272              iOS QA Engineer      Nothing found   \n",
       "4  1484   Litigation Support Analyst      Nothing found   \n",
       "\n",
       "                   company_name                    location  \\\n",
       "0              wildminds agency                 Windsor VIC   \n",
       "1                     uberbrand                  Sydney NSW   \n",
       "2  u and u Recruitment Partners  Sydney Western Suburbs NSW   \n",
       "3                     real time                 Adelaide SA   \n",
       "4                 people2people                  Sydney NSW   \n",
       "\n",
       "                                             summary  \\\n",
       "0  Like to push the boundaries with digital marke...   \n",
       "1  About this role You often read about roles bei...   \n",
       "2  Use your business analysis and technical BI sk...   \n",
       "3  About the companyReal Time Data RTD is a pione...   \n",
       "4  THE FIRM This global firm is a renowned law fi...   \n",
       "\n",
       "   Model Generalization Results  \n",
       "0                             1  \n",
       "1                             1  \n",
       "2                             2  \n",
       "3                             0  \n",
       "4                             1  "
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jobs_list['Model Generalization Results']=y_pred2\n",
    "jobs_list.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 2 - Part 3.2 - Feeding the data to a Pipeline to compare different models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1- Stochastic Gradient Descent (SGD)** \n",
    "\n",
    "SGD  applied to large-scale and sparse machine learning problems often encountered in text classification and natural language processing. Given that the data is sparse, the classifiers in this module easily scale to problems with more than 10^5 training examples and more than 10^5 features.\n",
    "\n",
    "**The advantages of Stochastic Gradient Descent are:**\n",
    " - Efficiency.\n",
    " - Ease of implementation (lots of opportunities for code tuning).\n",
    "\n",
    "**The disadvantages of Stochastic Gradient Descent include:**\n",
    " - SGD requires a number of hyperparameters such as the regularization parameter and the number of iterations.\n",
    " - SGD is sensitive to feature scaling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2- Logistic Regression** \n",
    "\n",
    "In the multiclass case, the training algorithm uses the one-vs-rest (OvR) scheme if the ‘multi_class’ option is set to ‘ovr’, and uses the cross-entropy loss if the ‘multi_class’ option is set to ‘multinomial’.\n",
    "\n",
    "The ‘liblinear’ solver supports both L1 and L2 regularization, with a dual formulation only for the L2 penalty. The Elastic-Net regularization is only supported by the ‘saga’ solver.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=jobs_with_salary_cat.summary\n",
    "# y=jobs_with_salary_cat.iloc[:,6].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing grid search...\n",
      "pipeline: ['vect', 'tfidf', 'clf']\n",
      "parameters:\n",
      "{'clf__alpha': (1e-05, 1e-06),\n",
      " 'clf__max_iter': (20,),\n",
      " 'clf__penalty': ('l2', 'elasticnet'),\n",
      " 'vect__max_df': (0.5, 0.75, 1.0),\n",
      " 'vect__ngram_range': ((1, 1), (1, 2), (1, 3))}\n",
      "Fitting 5 folds for each of 36 candidates, totalling 180 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:   21.9s\n",
      "[Parallel(n_jobs=4)]: Done 180 out of 180 | elapsed:  1.5min finished\n",
      "/Users/sherf/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n",
      "/Users/sherf/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/stochastic_gradient.py:603: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  ConvergenceWarning)\n",
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 93.156s\n",
      "\n",
      "Best score: 0.484\n",
      "Best parameters set:\n",
      "\tclf__alpha: 1e-06\n",
      "\tclf__max_iter: 20\n",
      "\tclf__penalty: 'l2'\n",
      "\tvect__max_df: 1.0\n",
      "\tvect__ngram_range: (1, 3)\n",
      "________________________________________________________\n",
      "Performing grid search...\n",
      "pipeline: ['vect', 'tfidf', 'clf2']\n",
      "parameters:\n",
      "{'clf2__max_iter': (20,),\n",
      " 'vect__max_df': (0.5, 0.75, 1.0),\n",
      " 'vect__ngram_range': ((1, 1), (1, 2), (1, 3))}\n",
      "Fitting 5 folds for each of 9 candidates, totalling 45 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done  45 out of  45 | elapsed: 19.2min finished\n",
      "/Users/sherf/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n",
      "/Users/sherf/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/Users/sherf/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/Users/sherf/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/Users/sherf/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/Users/sherf/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 1154.717s\n",
      "\n",
      "Best score: 0.494\n",
      "Best parameters set:\n",
      "\tclf2__max_iter: 20\n",
      "\tvect__max_df: 0.75\n",
      "\tvect__ngram_range: (1, 1)\n"
     ]
    }
   ],
   "source": [
    "# Define a pipeline combining a text feature extractor with a simple\n",
    "# classifier\n",
    "from pprint import pprint\n",
    "from time import time\n",
    "import logging\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.linear_model import SGDClassifier, LogisticRegressionCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', SGDClassifier(tol=1e-3)),\n",
    "])\n",
    "\n",
    "pipeline2 = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf2', LogisticRegressionCV(cv=5, random_state=0,multi_class='multinomial')),\n",
    "])\n",
    "# uncommenting more parameters will give better exploring power but will\n",
    "# increase processing time in a combinatorial way\n",
    "parameters = {\n",
    "    'vect__max_df': (0.5, 0.75, 1.0),\n",
    "    # 'vect__max_features': (None, 5000, 10000, 50000),\n",
    "    'vect__ngram_range': ((1, 1), (1, 2),(1,3)),  # unigrams or bigrams\n",
    "    # 'tfidf__use_idf': (True, False),\n",
    "    # 'tfidf__norm': ('l1', 'l2'),\n",
    "    'clf__max_iter': (20,),\n",
    "    'clf__alpha': (0.00001, 0.000001),\n",
    "    'clf__penalty': ('l2', 'elasticnet'),\n",
    "    # 'clf__max_iter': (10, 50, 80),\n",
    "}\n",
    "\n",
    "\n",
    "parameters2 = {\n",
    "    'vect__max_df': (0.5, 0.75, 1.0),\n",
    "    # 'vect__max_features': (None, 5000, 10000, 50000),\n",
    "    'vect__ngram_range': ((1, 1), (1, 2),(1,3)),  # unigrams or bigrams\n",
    "    # 'tfidf__use_idf': (True, False),\n",
    "    # 'tfidf__norm': ('l1', 'l2'),\n",
    "    'clf2__max_iter': (20,),\n",
    "#     'clf2__alpha': (0.00001, 0.000001),\n",
    "    #'clf2__penalty': (l2),\n",
    "    # 'clf__max_iter': (10, 50, 80),\n",
    "}\n",
    "\n",
    "# multiprocessing requires the fork to happen in a __main__ protected\n",
    "# block\n",
    "\n",
    "# find the best parameters for both the feature extraction and the\n",
    "# classifier\n",
    "grid_search1 = GridSearchCV(pipeline, parameters, cv=5,\n",
    "                       n_jobs=4, verbose=1)\n",
    "\n",
    "print(\"Performing grid search...\")\n",
    "print(\"pipeline:\", [name for name, _ in pipeline.steps])\n",
    "print(\"parameters:\")\n",
    "pprint(parameters)\n",
    "t0 = time()\n",
    "grid_search1.fit(X.tolist(), y)\n",
    "print(\"done in %0.3fs\" % (time() - t0))\n",
    "print()\n",
    "\n",
    "print(\"Best score: %0.3f\" % grid_search1.best_score_)\n",
    "print(\"Best parameters set:\")\n",
    "best_parameters = grid_search1.best_estimator_.get_params()\n",
    "\n",
    "for param_name in sorted(parameters.keys()):\n",
    "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n",
    "    \n",
    "print('________________________________________________________')\n",
    "\n",
    "grid_search2 = GridSearchCV(pipeline2, parameters2, cv=5,\n",
    "                       n_jobs=4, verbose=1)\n",
    "\n",
    "print(\"Performing grid search...\")\n",
    "print(\"pipeline:\", [name for name, _ in pipeline2.steps])\n",
    "print(\"parameters:\")\n",
    "pprint(parameters2)\n",
    "t0 = time()\n",
    "grid_search.fit(X.tolist(), y)\n",
    "print(\"done in %0.3fs\" % (time() - t0))\n",
    "print()\n",
    "\n",
    "print(\"Best score: %0.3f\" % grid_search.best_score_)\n",
    "print(\"Best parameters set:\")\n",
    "best_parameters = grid_search.best_estimator_.get_params()\n",
    "\n",
    "for param_name in sorted(parameters2.keys()):\n",
    "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 3 - Job Catagory Factors - Topic Modeling using `Latent Dirichlet Allocation (LDA) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part, I will try to find the factors that distinguish job category using LDA model to map all the documents to the topics that best describe the job catagories."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Section 3 - Part 1 - Prepropcessing**\n",
    "\n",
    "*Step 1: Convert documents to a single list/matrix.*\n",
    "\n",
    "*Step 2: Tokenization to break text into tokens using the help og regualer expressions.*\n",
    "\n",
    "*Step 3: Stemming the words into roots*\n",
    "\n",
    "*Step 4: Stop words removal*\n",
    "\n",
    "*Step 5: Apply to all documents*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora, models, matutils\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from collections import defaultdict\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles=jobs_list['job_title'].values\n",
    "titles=titles.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "stop_words = list(ENGLISH_STOP_WORDS)\n",
    "tokenizer = RegexpTokenizer('^\\D+')\n",
    "p_stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['LEGEND SEM   PPC Specialist']"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1=titles[0]\n",
    "tokenizer.tokenize(t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list for tokenized documents in loop\n",
    "texts = []\n",
    "\n",
    "# loop through document list\n",
    "for i in titles:\n",
    "    \n",
    "    # clean and tokenize document string\n",
    "    raw = i.lower()\n",
    "    tokens = tokenizer.tokenize(raw)\n",
    "\n",
    "    # remove stop words from tokens\n",
    "    stopped_tokens = [i for i in tokens if not i in stop_words]\n",
    "    \n",
    "    # stem tokens\n",
    "    stemmed_tokens = [p_stemmer.stem(i) for i in stopped_tokens]\n",
    "    \n",
    "    # add tokens to list\n",
    "    texts.append(stemmed_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Section 3 - Part 2 - Running LDA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn our tokenized documents into a id <-> term dictionary\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "    \n",
    "# convert tokenized documents into a document-term matrix\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "\n",
    "# generate LDA model\n",
    "ldamodel = gensim.models.LdaModel(corpus, num_topics=4, id2word = dictionary, passes=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, '0.006*\"marketing analyst\" + 0.006*\"senior security analyst\" + 0.005*\"senior data engin\" + 0.005*\"process engin\"'), (2, '0.009*\"cyber security analyst\" + 0.008*\"senior business analyst\" + 0.007*\"electronics engin\" + 0.007*\"data scientist   educ\"')]\n"
     ]
    }
   ],
   "source": [
    "print(ldamodel.print_topics(num_topics=2, num_words=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(2, '0.009*\"cyber security analyst\" + 0.009*\"information security analyst\"'), (0, '0.009*\"online analyst – saba  sportscraft   jag\" + 0.006*\"bi develop\"')]\n"
     ]
    }
   ],
   "source": [
    "print(ldamodel.print_topics(num_topics=2, num_words=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Trying different LDA Models to find a better results***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/sherf/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stemmer = SnowballStemmer('english')\n",
    "# original_words = ['caresses', 'flies', 'dies', 'mules', 'denied','died', 'agreed', 'owned', \n",
    "#            'humbled', 'sized','meeting', 'stating', 'siezing', 'itemization','sensational', \n",
    "#            'traditional', 'reference', 'colonizer','plotted']\n",
    "# singles = [stemmer.stem(plural) for plural in original_words]\n",
    "# pd.DataFrame(data = {'original word': original_words, 'stemmed': singles})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_stemming(text):\n",
    "    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
    "\n",
    "def preprocess(text):\n",
    "    result = []\n",
    "    for token in gensim.utils.simple_preprocess(text):\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n",
    "            result.append(lemmatize_stemming(token))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original document: \n",
      "['Associate', 'Relationship', 'Manager', '', 'Infrastructure', '', 'Energy', 'and', 'Resources', '', 'CIB']\n",
      "\n",
      "\n",
      " tokenized and lemmatized document: \n",
      "['associ', 'relationship', 'manag', 'infrastructur', 'energi', 'resourc', 'cib']\n"
     ]
    }
   ],
   "source": [
    "doc_sample = titles[101]\n",
    "\n",
    "print('original document: ')\n",
    "words = []\n",
    "for word in doc_sample.split(' '):\n",
    "    words.append(word)\n",
    "print(words)\n",
    "print('\\n\\n tokenized and lemmatized document: ')\n",
    "print(preprocess(doc_sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents=jobs_list['summary']\n",
    "processed_docs = documents.map(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = gensim.corpora.Dictionary(processed_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 abil\n",
      "1 adword\n",
      "2 agenc\n",
      "3 analysi\n",
      "4 analyt\n",
      "5 bid\n",
      "6 bing\n",
      "7 bonus\n",
      "8 boundari\n",
      "9 budget\n",
      "10 busi\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for k, v in dictionary.iteritems():\n",
    "    print(k, v)\n",
    "    count += 1\n",
    "    if count > 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary.filter_extremes(no_below=10, no_above=0.5, keep_n=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_corpus = [dictionary.doc2bow(doc) for doc in processed_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bow_doc_4310 = bow_corpus[100]\n",
    "\n",
    "# for i in range(len(bow_doc_4310)):\n",
    "#     print(\"Word {} (\\\"{}\\\") appears {} time.\".format(bow_doc_4310[i][0], \n",
    "#                                                      dictionary[bow_doc_4310[i][0]], \n",
    "#                                                      bow_doc_4310[i][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tfidf = models.TfidfModel(bow_corpus)\n",
    "# corpus_tfidf = tfidf[bow_corpus]\n",
    "# from pprint import pprint\n",
    "\n",
    "# for doc in corpus_tfidf:\n",
    "#     pprint(doc)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = gensim.models.LdaMulticore(bow_corpus, num_topics=10, id2word=dictionary, passes=20, workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 \n",
      "Words: 0.014*\"custom\" + 0.011*\"research\" + 0.009*\"client\" + 0.009*\"market\" + 0.009*\"design\" + 0.008*\"product\" + 0.006*\"analysi\" + 0.006*\"report\" + 0.005*\"secur\" + 0.005*\"australia\"\n",
      "Topic: 1 \n",
      "Words: 0.012*\"market\" + 0.007*\"client\" + 0.007*\"report\" + 0.006*\"inform\" + 0.006*\"secur\" + 0.006*\"perform\" + 0.006*\"time\" + 0.005*\"risk\" + 0.005*\"analyst\" + 0.005*\"oper\"\n",
      "Topic: 2 \n",
      "Words: 0.011*\"custom\" + 0.007*\"client\" + 0.006*\"report\" + 0.006*\"peopl\" + 0.006*\"compani\" + 0.005*\"intellig\" + 0.005*\"industri\" + 0.005*\"australia\" + 0.005*\"thale\" + 0.005*\"analysi\"\n",
      "Topic: 3 \n",
      "Words: 0.011*\"client\" + 0.010*\"custom\" + 0.007*\"market\" + 0.006*\"compani\" + 0.006*\"learn\" + 0.006*\"candid\" + 0.006*\"posit\" + 0.006*\"research\" + 0.005*\"employ\" + 0.005*\"technolog\"\n",
      "Topic: 4 \n",
      "Words: 0.011*\"technolog\" + 0.010*\"custom\" + 0.009*\"client\" + 0.009*\"product\" + 0.007*\"drive\" + 0.007*\"technic\" + 0.007*\"market\" + 0.006*\"consult\" + 0.006*\"build\" + 0.006*\"engin\"\n",
      "Topic: 5 \n",
      "Words: 0.011*\"report\" + 0.010*\"secur\" + 0.009*\"inform\" + 0.007*\"client\" + 0.007*\"analysi\" + 0.006*\"market\" + 0.006*\"peopl\" + 0.006*\"oper\" + 0.005*\"excel\" + 0.005*\"design\"\n",
      "Topic: 6 \n",
      "Words: 0.012*\"custom\" + 0.009*\"client\" + 0.008*\"peopl\" + 0.007*\"oper\" + 0.006*\"ensur\" + 0.006*\"posit\" + 0.006*\"plan\" + 0.006*\"perform\" + 0.005*\"product\" + 0.005*\"level\"\n",
      "Topic: 7 \n",
      "Words: 0.010*\"learn\" + 0.009*\"client\" + 0.008*\"research\" + 0.007*\"technolog\" + 0.006*\"world\" + 0.006*\"build\" + 0.006*\"peopl\" + 0.006*\"engin\" + 0.006*\"market\" + 0.005*\"compani\"\n",
      "Topic: 8 \n",
      "Words: 0.008*\"design\" + 0.008*\"product\" + 0.006*\"technolog\" + 0.006*\"implement\" + 0.006*\"report\" + 0.006*\"engin\" + 0.006*\"program\" + 0.006*\"technic\" + 0.006*\"stakehold\" + 0.006*\"model\"\n",
      "Topic: 9 \n",
      "Words: 0.010*\"custom\" + 0.009*\"client\" + 0.008*\"technolog\" + 0.008*\"sale\" + 0.007*\"technic\" + 0.007*\"product\" + 0.006*\"secur\" + 0.006*\"knowledg\" + 0.005*\"posit\" + 0.005*\"industri\"\n"
     ]
    }
   ],
   "source": [
    "for idx, topic in lda_model.print_topics(-1):\n",
    "    print('Topic: {} \\nWords: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 Word: 0.072*\"analyst\" + 0.063*\"data\" + 0.049*\"scientist\" + 0.039*\"analyt\" + 0.035*\"senior\" + 0.030*\"manag\" + 0.028*\"busi\" + 0.027*\"research\" + 0.025*\"consult\" + 0.023*\"secur\"\n",
      "Topic: 1 Word: 0.060*\"engin\" + 0.055*\"analyst\" + 0.047*\"develop\" + 0.042*\"busi\" + 0.041*\"manag\" + 0.036*\"senior\" + 0.035*\"data\" + 0.031*\"intellig\" + 0.027*\"consult\" + 0.022*\"market\"\n"
     ]
    }
   ],
   "source": [
    "lda_model_tfidf = gensim.models.LdaMulticore(corpus_tfidf, num_topics=2, id2word=dictionary, passes=2, workers=4)\n",
    "for idx, topic in lda_model_tfidf.print_topics(-1):\n",
    "    print('Topic: {} Word: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Score: 0.44329068064689636\t \n",
      "Topic: 0.020*\"technolog\" + 0.014*\"product\" + 0.013*\"custom\" + 0.011*\"technic\" + 0.011*\"learn\" + 0.011*\"engin\" + 0.010*\"softwar\" + 0.010*\"consult\" + 0.009*\"client\" + 0.009*\"design\"\n",
      "\n",
      "Score: 0.3367806077003479\t \n",
      "Topic: 0.018*\"engin\" + 0.012*\"oper\" + 0.010*\"technic\" + 0.010*\"system\" + 0.009*\"product\" + 0.009*\"industri\" + 0.007*\"time\" + 0.007*\"design\" + 0.007*\"ensur\" + 0.006*\"equip\"\n",
      "\n",
      "Score: 0.2168297916650772\t \n",
      "Topic: 0.021*\"report\" + 0.013*\"analysi\" + 0.011*\"stakehold\" + 0.011*\"analyst\" + 0.009*\"insight\" + 0.009*\"custom\" + 0.008*\"intellig\" + 0.008*\"excel\" + 0.008*\"perform\" + 0.007*\"deliv\"\n"
     ]
    }
   ],
   "source": [
    "for index, score in sorted(lda_model[bow_corpus[3]], key=lambda tup: -1*tup[1]):\n",
    "    print(\"\\nScore: {}\\t \\nTopic: {}\".format(score, lda_model.print_topic(index, 10)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "# topics_labels = {\n",
    "#    0: \"Data Analyst\",\n",
    "#    1: \"Data Scientest\",\n",
    "#    2: \"Security Analyst\"\n",
    "# }\n",
    "\n",
    "# doc_topics = [lda.get_document_topics(doc) for doc in corpus]\n",
    "\n",
    "# topic_data = []\n",
    "\n",
    "# for document_id, topics in enumerate(doc_topics):\n",
    "    \n",
    "#     document_topics = []\n",
    "    \n",
    "#     for topic, probability in topics:\n",
    "       \n",
    "#         topic_data.append({\n",
    "#             'document_id':  document_id,\n",
    "#             'topic_id':     topic,\n",
    "#             'topic':        topics_labels[topic],\n",
    "#             'probability':  probability\n",
    "#         })\n",
    "\n",
    "# topics_df = pd.DataFrame(topic_data)\n",
    "# topics_df.pivot_table(values=\"probability\", index=[\"document_id\", \"topic\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
